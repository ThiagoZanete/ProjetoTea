import os
import time
import shutil
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from tqdm.auto import tqdm

from nilearn import datasets
from nilearn.maskers import NiftiLabelsMasker
from nilearn.connectome import ConnectivityMeasure






BASE_DIR = '/content/drive/Shareddrives/Projeto_TEA/Dados_TEA_V2/ABIDE_pcp'
CSV_PATH = f'{BASE_DIR}/Phenotypic_V1_0b_preprocessed1.csv'
IMGS_DIR = f'{BASE_DIR}/cpac/filt_noglobal'

CACHE_TRAIN = '/content/fc_cache_train'
CACHE_VAL   = '/content/fc_cache_val'

DRIVE_TRAIN = f'{BASE_DIR}/fc_cache_train'
DRIVE_VAL   = f'{BASE_DIR}/fc_cache_val'

BATCH_SIZE = 16
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print(f"‚öôÔ∏è Pipeline FC no {DEVICE}")





#atlas + masker
atlas = datasets.fetch_atlas_harvard_oxford(#obt√©m o atlas de Harvard-Oxford, que √© um conjunto de regi√µes cerebrais pr√©-definidas usadas para an√°lise de conectividade funcional.
    'cort-maxprob-thr25-2mm'
)

masker = NiftiLabelsMasker(#O NiftiLabelsMasker √© uma ferramenta do Nilearn que extrai sinais m√©dios de regi√µes espec√≠ficas do c√©rebro, definidas por um atlas. Ele transforma os dados 4D (espa√ßo + tempo) em uma matriz 2D (regi√µes x tempo).
    labels_img=atlas.maps,
    standardize='zscore_sample',
    verbose=0
)

corr = ConnectivityMeasure(kind='correlation')#O ConnectivityMeasure calcula medidas de conectividade funcional, como correla√ß√£o, entre as regi√µes cerebrais. Ele transforma os sinais extra√≠dos pelo masker em uma matriz de conectividade (regi√µes x regi√µes), onde cada elemento representa a for√ßa da conex√£o entre duas regi√µes.






#mapeamento

df = pd.read_csv(CSV_PATH)#L√™ o arquivo CSV contendo os dados fenot√≠picos dos sujeitos

df['FILE_ID'] = (#Limpa a coluna FILE_ID, removendo espa√ßos e substituindo 'no_filename' por vazio
    df['FILE_ID']
    .astype(str)
    .str.strip()
    .str.replace('no_filename', '')
)

label_map = {#Cria um dicion√°rio que mapeia cada FILE_ID para um r√≥tulo bin√°rio (1 para DX_GROUP == 1, 0 caso contr√°rio)
    row.FILE_ID: int(row.DX_GROUP == 1)
    for _, row in df.iterrows()
    if row.FILE_ID != 'nan'
}


subjects = []

for fname in os.listdir(IMGS_DIR):#Percorre os arquivos na pasta de imagens pr√©-processadas, filtrando apenas os arquivos NIfTI e associando cada arquivo a um r√≥tulo usando o label_map criado anteriormente.
    if not fname.endswith(('.nii', '.nii.gz')):
        continue

    file_id = fname.replace('_func_preproc.nii.gz', '').replace('_func_preproc.nii', '')

    if file_id in label_map:#Se o FILE_ID extra√≠do do nome do arquivo existir no label_map, adiciona um dicion√°rio com o ID, caminho e r√≥tulo do sujeito √† lista de sujeitos v√°lidos.
        subjects.append({
            "id": file_id,
            "path": os.path.join(IMGS_DIR, fname),
            "label": label_map[file_id]
        })

print(f"üìÇ Sujeitos v√°lidos: {len(subjects)}")








#split dos dados em treino e valida√ß√£o, garantindo que a propor√ß√£o de r√≥tulos seja mantida em ambos os conjuntos.
train_subj, val_subj = train_test_split(
    subjects,
    test_size=0.2,
    random_state=42,
    stratify=[s["label"] for s in subjects]
)





#cria√ß√£o e salvamento do dataset no drive 
class ABIDEFCDataset(Dataset):
    def __init__(self, subjects, cache_dir, drive_dir=None):#O construtor do dataset recebe a lista de sujeitos, o diret√≥rio de cache para salvar os arquivos processados e um diret√≥rio opcional no Google Drive para sincroniza√ß√£o. Ele cria os diret√≥rios necess√°rios e chama o m√©todo _process_missing para processar os sujeitos que ainda n√£o foram processados.
        self.subjects = subjects
        self.cache_dir = cache_dir
        self.drive_dir = drive_dir

        os.makedirs(cache_dir, exist_ok=True)#Cria o diret√≥rio de cache local, se n√£o existir
        if drive_dir:
            os.makedirs(drive_dir, exist_ok=True)

        self._process_missing()

    def _process_missing(self):#Este m√©todo percorre os sujeitos e verifica se o arquivo processado j√° existe no cache. Se n√£o existir, ele processa o sujeito usando o masker para extrair os sinais m√©dios das regi√µes cerebrais e o corr para calcular a matriz de conectividade. A matriz √© ent√£o redimensionada para 64x64 usando interpola√ß√£o bilinear e salva como um tensor PyTorch. Se ocorrer algum erro durante o processamento, um tensor de zeros √© salvo em vez disso. Ap√≥s salvar o arquivo localmente, ele √© sincronizado com o Google Drive, se um diret√≥rio de drive for fornecido.
        for subj in tqdm(self.subjects, desc="Processando FC"):
            sid = subj["id"]
            cache_file = f"{self.cache_dir}/{sid}.pt"

            if os.path.exists(cache_file):
                continue

            try:
                ts = masker.fit_transform(subj["path"])#Usa atlas para extrair a atividade m√©dia de cada regi√£o ao longo do temp, dessa forma trransforma o v√≠deo 4D em uma tabela de sinais.
                mat = corr.fit_transform([ts])[0]#Calcula a matriz de correla√ß√£o. Se a regi√£o A sobe atividade quando a B sobe, a conex√£o √© forte.
                np.fill_diagonal(mat, 0)

                tensor = torch.from_numpy(mat).float().unsqueeze(0)#Converte a matriz de conectividade em um tensor PyTorch 
                tensor = F.interpolate(#s√≥ para redimensionar a matriz para 64x64
                    tensor.unsqueeze(0),
                    size=(64, 64),
                    mode="bilinear"
                ).squeeze(0)

            except Exception as e:
                print(f"‚ö†Ô∏è {sid}: {e}")
                tensor = torch.zeros((1, 64, 64))

            torch.save((tensor, subj["label"]), cache_file)

            if self.drive_dir:
                self._sync(cache_file)

    def _sync(self, local_file):#Este m√©todo sincroniza um arquivo local com o Google Drive. Ele constr√≥i o caminho de destino no drive e copia o arquivo local para l√°, se ele ainda n√£o existir. Se ocorrer algum erro durante a c√≥pia, ele √© ignorado.
        dst = os.path.join(self.drive_dir, os.path.basename(local_file))
        if os.path.exists(dst):
            return
        try:
            shutil.copyfile(local_file, dst)
        except Exception:
            pass

    def __len__(self):
        return len(self.subjects)

    def __getitem__(self, idx):
        sid = self.subjects[idx]["id"]
        return torch.load(f"{self.cache_dir}/{sid}.pt")









train_dataset = ABIDEFCDataset(#Cria o dataset de treino usando a classe ABIDEFCDataset, passando os sujeitos de treino, o diret√≥rio de cache para treino e o diret√≥rio de drive para treino.
    train_subj,
    cache_dir=CACHE_TRAIN,
    drive_dir=DRIVE_TRAIN
)

val_dataset = ABIDEFCDataset(#Cria o dataset de valida√ß√£o usando a classe ABIDEFCDataset, passando os sujeitos de valida√ß√£o, o diret√≥rio de cache para valida√ß√£o e o diret√≥rio de drive para valida√ß√£o.
    val_subj,
    cache_dir=CACHE_VAL,
    drive_dir=DRIVE_VAL
)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE)
